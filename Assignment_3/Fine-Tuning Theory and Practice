Assignment: Fine-Tuning Theory and Practice
Part 1: Theory of Fine-Tuning
Concept Check (Multiple Choice Questions)
Q1: What is the main benefit of fine-tuning an LLM?
Correct Answer: B) It customizes the model for specific tasks or domains.

Q2: Which of the following describes "catastrophic forgetting"?
Correct Answer: B) When the model loses its generalization ability after excessive fine-tuning on a specific task.

Application Task: Transfer Learning Explanation
Transfer Learning Analogy: Learning a New Sport
Transfer learning in AI is like learning a new sport after mastering another. Imagine a professional tennis player who wants to start playing badminton. Because both sports involve hand-eye coordination, agility, and racket handling, the player doesn’t have to start from scratch. Instead, they transfer their existing skills (muscle memory, strategy, reaction time) to badminton while adapting to the new rules and techniques.

Similarly, in machine learning, a pre-trained model (like BERT or GPT) is trained on vast amounts of text data and can be fine-tuned for specific tasks (e.g., sentiment analysis, legal document classification) with minimal additional training. This allows AI to quickly adapt to specialized tasks without requiring vast amounts of new labeled data.

Example Dataset Structure for Fine-Tuning a Sentiment Analysis Model
For fine-tuning DistilBERT on sentiment classification (e.g., classifying product reviews as positive or negative), the dataset can be structured as follows:
Review Text	Sentiment
"This product is amazing! Works perfectly and exceeded expectations."	Positive
"Terrible experience. It broke within a week. Do not recommend."	Negative
"Average quality, nothing special but does the job."	Neutral
"I absolutely love it! Best purchase I've made."	Positive
"Completely useless. Waste of money."	Negative

Cleaning the Dataset:
Removing special characters & HTML tags
Lowercasing text
Tokenizing and padding
Removing duplicate reviews
Part 2: Practical Fine-Tuning Session
We will fine-tune distilbert-base-uncased on text classification using Hugging Face.

1. Environment Setup
Install required libraries:

bash
Copy code
pip install transformers datasets torch scikit-learn
Check GPU availability:

python
Copy code
import torch
print("GPU available:", torch.cuda.is_available())
2. Preprocessing Data (IMDB Dataset for Sentiment Analysis)
python
Copy code
from datasets import load_dataset
from transformers import DistilBertTokenizer

# Load dataset
dataset = load_dataset("imdb")

# Initialize tokenizer
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")

# Tokenization function
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

# Tokenize dataset
tokenized_datasets = dataset.map(tokenize_function, batched=True)
3. Fine-Tuning the Model
python
Copy code
from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments

# Load pre-trained model
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)

# Define training arguments
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
)

# Define Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"].shuffle(seed=42).select(range(2000)),
    eval_dataset=tokenized_datasets["test"].shuffle(seed=42).select(range(500)),
)

# Train the model
trainer.train()
4. Save and Evaluate the Model
python
Copy code
# Save fine-tuned model
model.save_pretrained("./fine_tuned_distilbert")
tokenizer.save_pretrained("./fine_tuned_distilbert")

# Evaluate accuracy
from sklearn.metrics import accuracy_score

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = predictions.argmax(axis=1)
    return {"accuracy": accuracy_score(labels, predictions)}

trainer.evaluate()
Reflection: Challenges and Improvements
Challenges Faced:
Limited Data:
Using only 2000 training samples instead of the full dataset could reduce generalization.
Computational Constraints:
Fine-tuning on a large dataset requires GPU acceleration for faster training.
Hyperparameter Sensitivity:
Finding the right learning rate, batch size, and epochs was crucial for optimal performance.
Suggestions for Improvement (If Accuracy < 90%)
Increase Training Data → Using more labeled samples improves generalization.
Experiment with Learning Rates → Trying different values (e.g., 2e-5, 5e-5) can improve convergence.
Use Data Augmentation → Synonym replacement and paraphrasing help reduce bias.
Train for More Epochs → Increasing epochs from 3 to 5 could enhance learning.

